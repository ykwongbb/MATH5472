\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in, total={105ex, 150ex}]{geometry}
\usepackage{graphicx}
\usepackage[parfill]{parskip}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{nicefrac}
\usepackage{yfonts}
\usepackage[cal=stixtwofancy]{mathalpha}
\usepackage{caption}
\usepackage{subfig}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[nodayofweek,UKenglish]{datetime}

\setcounter{section}{0}
\renewcommand{\baselinestretch}{1.5}
\addtolength{\skip\footins}{1.5em}
\AtBeginDocument{{\footnotesize\global\footnotesep=0.75\baselineskip}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\allowdisplaybreaks

\title{\textbf{Summary and discussion of: ``A Fast Coordinate Descent Method for High-Dimensional Non-Negative Least Squares using a Unified Sparse Regression Framework'' \footnote{The Latex document and the reproduced code can be found in \url{https://github.com/ykwongbb/MATH5472}}}\\} 
\author{WONG, Yiu Kwan}
\date{15/12/2024}

\begin{document}
\maketitle
\tableofcontents

\section{Overview of the paper} 
\subsection{Why is it Interesting}
This paper is interesting because it introduces a unifying framework that connects several 
well-known methods, such as NNLS, BVLS, and Lasso, under the general problem -- polyhedron 
constrained least squares. In particularly, the paper shows that the problem admits locally
unique sparse solution in high dimensions. Also, the paper gives both theoretical understanding 
and applications on NNLS, BVLS, Lasso, etc. of these constrained regression problems.
Moreover, the paper introduces a novel algorithm based on coordinate descent, which shown 
to be at least a 5x speed-up from the state-of-the-art solvers. In summary, the paper is interesting 
as it addresses both the theoretical aspect and the algorithmic aspects of constrained sparsed 
regression problems in high-dimensional spaces, which are important in statistical machine learning.


\subsection{Challenges}
\textbf{High Dimensionality}

In modern applications, the number of features $p$ can be much larger than the number of samples
$n$, leading to a high-dimensional setting. However, traditional methods only performs well when
$n$ is relatively large to $p$. This makes the solutions of the standard least squares become 
ill-posed or overfitted.

\textbf{Local Uniqueness}

In the context of high-dimensionality, solutions to constrained least squares problems are usually
not unique and only locally unique in some subspaces. It is non-trivial to identify the necessary 
conditions for a subspace defined by the binding constraints.

\textbf{Constraints}

The coefficients in different problems are usually by some constraints, such as non-negativity or
certain bounds. This increases the computational complexity of the optimization problem and makes 
it more difficult to find a general solution that can handle different constraints.

% Motivation and Prior Methods

% The motivation behind this paper is to address the **limitations of existing methods** for constrained least squares problems in high-dimensional settings. Specifically:

% - **Existing methods**, such as those for NNLS and Lasso, often handle constraints independently, without considering the deeper connections between them. This leads to **inefficiencies** and a lack of **theoretical understanding** of how solutions behave under various constraint settings.
% - Prior approaches, like **active-set methods** for NNLS or **coordinate descent** for Lasso, while effective in certain cases, often fail to fully exploit the **sparsity** and **local uniqueness** of solutions, especially in problems with more complex constraint structures (e.g., BVLS). These methods are also not always efficient in high-dimensional settings, where computational speed is critical.

% Before this paper, the focus was typically on solving each constrained regression problem independently,
% without a unified framework that could address a range of polyhedral constraints. Moreover, while some
% methods could guarantee sparsity (e.g., Lasso), they did not provide guarantees of **local uniqueness** in the solution, which is important for **interpretability** and **certainty** in the results.


\subsection{Prposed methods}
\textbf{Definition 3.1}\quad A \textbf{polyhedron} is defined as 
\begin{equation}
    \mathcal{P} \equiv \{x \in \mathbb{R}^p : Ax \leq b \}
\end{equation} 
where $A \in \mathbb{R}^{m \times p}$ and $b \in \mathbb{R}^{m}$. An element $x \in \mathcal{P}$ 
binds the $i$-th constraint if $a_i^T x = b_i$. We say $x$ binds $k$ constraints if there exists 
a set $S \subseteq [m] = \{1, \cdots, m\}$ of size $k$ such that $x$ binds exactly these $k$ 
constraints for the $i$-th constraint for every $i \in S$. Equivalently, we can express $\mathcal{P}$ 
as
\begin{equation}
    \mathcal{P} \equiv \{x \in \mathbb{R}^p : A_{-S}x \leq b_{-S}, A_{S}x = b_{S} \}
\end{equation} 
where $S = \{i \in [m]: a_i^T x = b_i, \forall x \in \mathcal{P}\}$, $A_{S}$ are the rows of $A$ 
indexed by $S$ and $A_{-S}$ are the rows of $A$ indexed by $[m] \setminus S$. As such, we have a 
non-empty interior $\{A_{-S}x \leq b_{-S}\}$ and a uniquely determined affine space 
$\{A_{S}x = b_{S}\}$. If $\mathcal{P}$ is non-empty, then we denote the dimension of $\mathcal{P}$ 
as $\text{dim}(\mathcal{P}) = p - \text{rank}(A_S)$ and say $S$ is the set of maximal affine 
constraints.

\textbf{Definition 3.2}\quad  The \textbf{Hull} $\pmb{(Q, \mathcal{P})}$ is defined to be
\begin{equation}
    \text{Hull}(Q, \mathcal{P}) = \{ x \in \mathbb{R}^n : x = Qw, w \in \mathcal{P} \}.
\end{equation}
where  $Q \in \mathbb{R}^{n \times p} $ and $\mathcal{P} \subseteq \mathbb{R}^{p}$.
% If P is empty, we take Hull(Q,P) ≡ Rn. An element of Hull(Q,P) is called a P-combination
% ofQ. 
We say that $x \in \text{Hull}(Q, \mathcal{P})$ is a $\mathcal{P}$-combination of $Q$ with 
$w \in \mathcal{P}$ if $x = Qw$.

The paper shows that there always exists a solution to the polyhedral regression
\begin{equation}\label{pr}
    \min_{\beta \in \mathcal{P}} \frac{1}{2} \| y - X \beta \|_2^2,
\end{equation}
that binds the constraints of $A$. Also, the paper shows show the such a solution is unique when 
it isrestricted to the affine subspace defined by the binding constraints, which is the local 
uniqueness property.

\textbf{High-level description of the theoratical results}


The paper first establishes Lemma 3.3 quantifies the number of binding constraints for polyhedrons 
with non-empty interior point, then it is extended to a more general case by removing the assumption
of having non-empty interior in Therorem 3.4. From Theorem 3.4, we can immediately obtain the result
od Collary 3.5, which is the polyhedral regression. Finally, the paper ends this session by stating
the Theorem 3.6, which is the local uniqueness property.
For the Lemma, Collary and Theorems below, we will use the following notation: let 
$Q \in \mathbb{R}^{n \times p}$ be any matrix, $\mathcal{P} \subseteq \mathbb{R}^{p}$ be any polyhedron
represented by $A \in \mathbb{R}^{m \times p}$ and $b \in \mathbb{R}^{m}$. Also, let $S$ be the set of
maximal affine constraints and $x \in \text{Hull}(Q, \mathcal{P})$.

\textbf{Lemma 3.3}\quad 
If $\mathcal{P}$ has a non-empty interior and 
$\text{Null}(Q)\setminus\{\pmb{0}\}\subseteq\text{Null}(A)^c$, then $x$ can be written as a 
$\mathcal{P}$-combination of $Q$ with an element $w \in \mathcal{P}$ that binds at least 
$\min(\text{rank}(A), (p - n)_+)$ constraints of $A$.

The lemma is particularly useful when $p > n$ since it views the facet of the polyhedron as a lower
dimensional space. Intuitively, after restricting ourselves from $\mathcal{P}$ to $Q$, we remove 
$p - n$ degrees of freedom, as long as there are enough i.e. $\text{rank}(A) > p - n$ to be removed. 
Mathematically, $\text{Null}(Q)\setminus\{\pmb{0}\}\subseteq\text{Null}(A)^c$ ensures that the
element in $\text{Null}(Q)$ must not be ortgonal to the constraints of $A$, while the non-empty 
interior condition ensures the line intersects $\mathcal{P}$ at more than one point.

\newpage
\textbf{Theorem 3.4} (Existence of sparse representation)\quad 
If $V \in \mathbb{R}^{p \times \text{dim}(\mathcal{P})}$ is afull column rank matrix such that 
$\text{Null}(QV)\setminus\{\pmb{0}\}\subseteq\text{Null}(A_SV)^c$, then
$x \in \text{Hull}(Q, \mathcal{P})$ can be written as a $\mathcal{P}$-combination of $Q$ with an 
element $w \in \mathcal{P}$ that binds at least 
$\min(\text{rank}(A) - \text{rank}(A_S), \text{dim}(\mathcal{P}) - \text{rank}(QV))$ constraints 
of $A_{-S}$.

The theorem first takes away the assumption of non-empty interior stated in lemma 3.3, which is 
done by expressing the polyhedron in terms of the non-empty interior $\{A_{-S}x \leq b_{-S}\}$. 
After having the constrainted matrix $A_{-S}V$, with $\text{rank}(A_{-S}V) = \text{rank}(A) - \text{rank}(A_S)$,
the theorem also shows $QV$ is the full rank matrix of $A_{-S}V$, such that we can obtain 
$\text{dim}(\mathcal{P}) - \text{rank}(QV)$ and follow the result in lemma 3.3.
% Suppose that $\text{Null}(QV)\setminus\{\pmb{0}\}\subseteq\text{Null}(A_SV)^c$.
% Let Q ∈ Rn×p be any matrix and P ⊆ Rp be any polyhedron represented by A ∈ Rm×p and b ∈ Rm with a non-empty interior. Suppose that Null(Q) \ {0} ⊆ Null(A)c. If x is in the P-hull of Q, then x can be written as a P-combination of Q with an element w that binds at least min(rank(A), (p − n)+) constraints of A that form a full row rank sub-matrix of A.

\textbf{Collary 3.5} (Polyhedral regression) \quad 
Let $X \in \mathbb{R}^{n \times p}$ be a feature matrix and $y\in \mathbb{R}^{n}$ be the response 
vector. Take $\mathcal{P}$ such that the conditions of Theorem 3.4 hold with $Q \equiv X$. Also, let 
$V \in \mathbb{R}^{p \times \text{dim}(\mathcal{P})}$ such that $A_SV = \pmb{0}$. Then, there exists 
a solution $\hat{\beta} \in \mathbb{R}^p$ to (\ref{pr}) that binds at least 
$\min(\text{rank}(A) - \text{rank}(A_S), \text{dim}(\mathcal{P}) - \text{rank}(XV))$ 
constraints of $A_{-S}$.

This collary can be immediately obtained from Theorem 3.4, since we have
$X\beta = \Pi_{\text{Hull}(X, \mathcal{P})}(y) \in \text{Hull}(X, \mathcal{P})$, where the 
projection of the hull is closed and convex.

\textbf{Theorem 3.6} (Local uniqueness property)\quad 
Suppose the conditions of Theorem 3.4 hold and
$$
\text{rank}(A) - \text{rank}(A_S) \geq \text{dim}(\mathcal{P}) - \text{rank}(QV)
$$
Let $w$ be the sparse representation that binds at least $\text{dim}(\mathcal{P}) - \text{rank}(QV)$ 
constraints of $A_{-S}$. Denote $(A_{-S})_T$ be the binding constraints, where $T$ is the set of 
indices. Let $\mathcal{T} = \{w \in \mathbb{R}^p : (A_{-S})_Tw = (b_{-S})_T\}$. Then, $w$ is the 
unique element in $\mathcal{P} \cap \mathcal{T}$ such that $x = Qw$.

This theorem shows that the sparse solution $w$ is unique when restricted to the subspace by
Theorem 3.5. The ineqaulity in the condition usually holds in practice since we have 
$\text{rank}(A) \geq p$. This theorem shows that the solution is unique when the binding 
constraints are identified correctly, which is particularly useful in optimization
algorithms as we can perform the active-set strategies rapidly and obtain the optimal solution 
upon convergence.

\textbf{Collary 3.7} (NNLS solution)\quad There exists a solution with at most rank($X$) 
positive value to the NNLS problem.

Take $A = -I$ and $b = \pmb{0}$. Since the conditions of collary 3.5 hold, we can say there 
exists  $\beta \in \mathbb{R^p}$ that binds at least $p - \text{rank}(X)$ constraints of A, 
which is the result of Collary 3.7.
\newpage

\textbf{Collary 3.8} (BVLS solution)\quad There exists a solution with at most rank($X$) 
non-boundary value to the BVLS problem.

Take $A = \begin{bmatrix}
            -I\\
            I
         \end{bmatrix}$
and $b = \begin{bmatrix}
            -l \\
            u
        \end{bmatrix}$. 
Since the conditions of collary 3.5 hold, we can say there 
exists  $\beta \in \mathbb{R^p}$ that binds at least $p - \text{rank}(X)$ constraints of A, 
which is the result of Collary 3.8.

\textbf{Coordinate Descent for BVLS} 

Let $X \in R^{n \times p}$ and $y \in R^n$ be the feature matrix and response vector respectively,
and denote $l$ and $u$ as the lower and upper bounds. The NNLS problem is just setting $l = 0$ 
and $u = \infty$. The algorithm is intended to solve the BVLS problem in a high dimentional case 
i.e. $p >> n$, we have Collary 3.8 that guarantees the solution is unique and sparse. The 
algorithm is based on the coordinate descent, in which convergence is guaranteed [Tse01]. This 
is because coordinate descent is the state-of-the-art method in models like the lasso and group
lasso [YH24]. Therefore, it is expected that the algorithm can efficient and effective solve the 
sparse solution to the BVLS problem.

\textbf{High-level description of the BVLS algorithm} 

We first choose a subset of the features $S$, which requires adjustments the most. We will check
whether the current $\beta$ is optimal by evaluating the KKT conditions. If the KKT conditions are
not satisfied, we will solve the BVLS problem using $S$. This process is repeated iteratively 
until the KKT conditions are met or $S$ includes all features i.e. $S = [p]$. We will do this
by computing the violation($\delta_i$) of the KKT conditions as follows:
\begin{equation}
    \delta_i = \begin{cases}
        \max\{0, -\nabla_i f(\beta)\} & \text{if } \beta_i < u_i\\
        \max\{0, \nabla_i f(\beta)\} & \text{if } \beta > l_i\\
    \end{cases}
    \quad \forall i \in [p]
\end{equation}
We then sort the $\delta_i$ in descending order and choose the large violations, i.e. 
$\delta_i > 0$ in $S$ to update. Collary 3.8 shows that rank($X$) is always the upper bound
that guarantees the existence of a unique solution. However, if we do not know what rank($X$) is,
we can still estimate it by min($n$, $p$). After checking the KKT conditions on $S^c$, if we 
can obatin the optimal solution, which has violation equal to 0, then we are done. Otherwise,
we will solve the BVLS problem on $S$ and update $\beta$. We first perform one coordinate
descent on $S$ to identify the active set. If we are converged, then we are done. Otherwise, we
will do coordinate descent on the active set repeatedly until convergence. Also, in every 
iteration, we will remove the variables that have reached the boundary. We will set the initial
value of $\beta$ to be the vertex of the box that is closest to 0. This is because we want to
simulate the shrinkage effect in the penalized regression methods.

\section{Result and Discussion}
My implementation is based on the pseudocode of the BVLS algorithm in the paper and SciPy's 
implementation of the BVLS algorithm. In my implementation, I reproduce the result of paper's
algorithm and try compare the performance of the two algorithms by running them on the same 
dataset.

\subsection{Dataset}
I first generate non-negative i.i.d matrix $X$ uniformly on (0, 1) and set $80\%$ of the 
entries to be 0. This ensures us getting pairwise othogonal vector in a large probability. 
Then, i generate vector $y \sim \mathcal{N}(\mu,\sigma^{2})$, where 20 of the $\mu$ are equally 
chosen from [$-3\sigma, 3\sigma$] with $\sigma = 1$. Then I set $p = 1000$ and $p = 10000$ to 
see how large the dimension need to be in order to be consider high-dimensional. For both cases,
I set $n = 100$. For the NNLS problem, which is setting $l = 0$ and $u = \infty$. As for the BVLS
problem, I set $l = 0$ and $u = 1$.

\subsection{Result}
The result is shown in figures below, we can see the comparison between the runtime (a),
the objective value (b) and the size of the active set (c). For (a), we can see that in the 
case of $p = 1000$, the SciPy NNLS and BVLS always performs better than the paper's algorithm.
This could be simply understood as the dimension is not high enough to see the advantage of the
paper's algorithm. However, for $p = 10000$, we can see that the paper's algorithm is much faster 
than the SciPy's algorithm especially when $\mu$ is small. When $\mu$ is large, the paper's
algorithm is still faster than the SciPy's algorithm, but the difference is not as significant
as when $\mu$ is small. This is because when $\mu$ is large, the method of the paper will include
more variables than usual does. For (b), we can see that the objective value of both algorithms are the
same. For (c), we can see that the size of the active set for NNLS is fewer than that of BVLS,
which is expected since the NNLS problem has a larger bound than the BVLS problem. Also, we may 
see that the size of the active set gradually increases as $\mu$ increases, which is expected
since there are more min($n$, $p$) active variables.

\begin{figure}[H]
    \centering
    \hspace*{-0.8in}
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{NNLS_1000_time.png} \label{1}
    }
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{NNLS_1000_objective.png}\label{2}
    }
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{NNLS_1000_ActiveSet.png}\label{3}
    }
    \caption{NNLS for $n = 100$ and $p = 1000$}

    \hspace*{-0.8in}
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{BVLS_1000_time.png} \label{4}
    }
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{BVLS_1000_objective.png}\label{5}
    }
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{BVLS_1000_ActiveSet.png}\label{6}
    }
    \caption{BVLS for $n = 100$ and $p = 1000$}

    \hspace*{-0.8in}
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{NNLS_10000_time.png} \label{7}
    }
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{NNLS_10000_objective.png}\label{8}
    }
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{NNLS_10000_ActiveSet.png}\label{9}
    }
    \caption{NNLS for $n = 100$ and $p = 10000$}

    \hspace*{-0.8in}
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{BVLS_10000_time.png} \label{10}
    }
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{BVLS_10000_objective.png}\label{11}
    }
    \subfloat[]{
    \includegraphics[width=0.4\textwidth, height=0.252\textwidth]{BVLS_10000_ActiveSet.png}\label{12}
    }
    \caption{BVLS for $n = 100$ and $p = 10000$}
\end{figure}

\section{Conclusion}
In this paper, the authors introduce a unified framework for solving polyhedral constrained least
squares problems. They show that the problem admits locally unique sparse solutions in high
dimensions. The paper provides a theoretical understanding of the problem and presents a novel
algorithm based on coordinate descent. The results are promising and suggest that the proposed
algorithm can efficiently solve high-dimensional constrained regression problems.


\newpage

\begin{thebibliography}{5}
 
\bibitem{gentry}
    [Tse01] P. Tseng: \emph{Convergence of a block coordinate descent method for nondifferentiable 
    minimization}. Journal of Optimization Theory and Applications, 2001.

\bibitem{gentry}
    [YH24] J. Yang, T. Hastie: \emph{A fast and scalable pathwise-solver for group lasso and elastic
    net penalized regression via block-coordinate descent}, 2024. 
\end{thebibliography}

\end{document}

% ### **Section 3.1: Background**

% #### **Definition 3.1: Polyhedron**
% A **polyhedron** \( \mathcal{P} \subseteq \mathbb{R}^p \) is defined by a matrix \( A \in \mathbb{R}^{m \times p} \) and a vector \( b \in \mathbb{R}^m \), such that:
% \[
% \mathcal{P} = \{ x \in \mathbb{R}^p : A x \leq b \}.
% \]
% This represents a set of linear inequalities describing the polyhedron. A specific element \( x \in \mathcal{P} \) **binds** the \( i \)-th constraint if \( a_i^T x = b_i \), i.e., it satisfies the inequality as an equality. If there exists a set \( S \subseteq [m] \) of size \( k \) such that \( x \) binds exactly these \( k \) constraints, we say \( x \) binds \( k \) constraints.

% - Bound constraints signify that the solution lies on the boundary of the polyhedron.
% - **Non-trivial constraints** are those with non-zero coefficients \( a_i \neq 0 \).

% The **polyhedron** can be rewritten to explicitly separate the equality constraints:
% \[
% \mathcal{P} = \{ x \in \mathbb{R}^p : A_{-S} x \leq b_{-S}, A_S x = b_S \},
% \]
% where \( S \) is the set of indices for the equality constraints.

% #### **Definition 3.2: Dimension of a Polyhedron**
% The **dimension** of a polyhedron \( \mathcal{P} \) is defined as:
% \[
% \text{dim}(\mathcal{P}) = p - \text{rank}(A_S),
% \]
% where \( A_S \) denotes the matrix of maximal affine constraints, i.e., the set of equalities binding the polyhedron. Intuitively, the dimension measures the **degrees of freedom** in the polyhedron after accounting for the constraints.

% - If \( p - \text{rank}(A_S) > 0 \), there are **degrees of freedom** for the solution.
% - If \( p - \text{rank}(A_S) = 0 \), the solution is uniquely determined by the constraints.

% #### **Definition 3.3: Hull**
% The **P-hull** of a matrix \( Q \in \mathbb{R}^{n \times p} \) and a polyhedron \( \mathcal{P} \subseteq \mathbb{R}^p \) is defined as:
% \[
% \text{Hull}(Q, \mathcal{P}) := \{ x \in \mathbb{R}^n : x = Q w, w \in \mathcal{P} \}.
% \]
% This represents the set of all points that can be formed as a linear combination of the columns of \( Q \), where the coefficients \( w \) lie in the polyhedron \( \mathcal{P} \).

% - If \( \mathcal{P} \) is a polyhedron, the **P-hull** is also a polyhedron.
% - This concept generalizes the notion of **convex hulls** and **conical hulls**.

% ---

% ### **Section 3.2: Main Results**

% This section focuses on proving the existence of **sparse solutions** to polyhedral regression problems and establishing their **local uniqueness**.

% #### **Problem Setup:**
% The goal is to solve a **polyhedral regression problem** of the form:
% \[
% \min_{\beta \in \mathcal{P}} \frac{1}{2} \| y - X \beta \|_2^2,
% \]
% where \( X \in \mathbb{R}^{n \times p} \) is the feature matrix, \( y \in \mathbb{R}^n \) is the response vector, and \( \beta \in \mathcal{P} \) is constrained by the polyhedron \( \mathcal{P} \).

% The objective is to show that there exists a solution \( \beta \in \mathcal{P} \) that **binds many constraints** of \( A \), ensuring that the solution is **locally unique** when restricted to the subspace defined by these constraints.

% #### **Lemma 3.4: Existence of Binding Constraints**
% **Statement:**
% Let \( Q \in \mathbb{R}^{n \times p} \) be a matrix, and let \( \mathcal{P} \subseteq \mathbb{R}^p \) be a polyhedron represented by \( A \in \mathbb{R}^{m \times p} \) and \( b \in \mathbb{R}^m \). Suppose that \( \text{Null}(Q) \setminus \{0\} \subseteq \text{Null}(A)^c \). If \( x \in \text{Hull}(Q, \mathcal{P}) \), then \( x \) can be written as a **P-combination** of \( Q \) with an element \( w \in \mathcal{P} \) that binds at least \( \min(\text{rank}(A), (p - n)_+) \) constraints of \( A \).

% **Key Idea:**
% This lemma shows that if \( x \) is the result of a linear combination \( x = Qw \) for some \( w \in \mathcal{P} \), then there is a representation of \( x \) that binds a significant number of constraints in \( A \). This ensures that the solution is restricted to a **lower-dimensional subspace** of the polyhedron.

% - **Proof Sketch:**  
%   The proof involves moving in the **null space** of \( Q \) to reach the boundary of the polyhedron, where additional constraints become active, effectively reducing the degrees of freedom.

% #### **Theorem 3.5: Existence of a Sparse Representation**
% **Statement:**
% Let \( Q \in \mathbb{R}^{n \times p} \) be any matrix, and \( \mathcal{P} \subseteq \mathbb{R}^p \) be a polyhedron. Suppose that \( \text{Null}(QV) \setminus \{0\} \subseteq \text{Null}(A_SV)^c \). If \( x \in \text{Hull}(Q, \mathcal{P}) \), then \( x \) can be written as a **P-combination** of \( Q \) with an element \( w \in \mathcal{P} \) that binds at least:
% \[
% \min(\text{rank}(A) - \text{rank}(A_S), \text{dim}(\mathcal{P}) - \text{rank}(QV))
% \]
% constraints of \( A \).

% **Key Idea:**
% This theorem generalizes **Lemma 3.4** and formalizes the existence of a **sparse solution** to the polyhedral regression problem. It quantifies how many constraints are bound by the solution.

% - **Proof Sketch:**  
%   The proof builds on the idea of **dimensional reduction**: by moving along the null space of \( Q \), the solution is projected onto a lower-dimensional subspace where the polyhedron \( \mathcal{P} \) still has a non-empty interior. The result is a solution that binds a significant number of constraints, ensuring sparsity.

% #### **Corollary 3.6: Carathéodory's Theorem**
% This corollary provides an application of **Theorem 3.5** to **Carathéodory's Theorem**, which states that any point in a convex hull of \( p \) points can be written as a convex combination of at most \( p + 1 \) points.

% #### **Theorem 3.8: Local Uniqueness Property**
% **Statement:**
% Suppose that:
% \[
% \text{rank}(A) - \text{rank}(A_S) \geq \text{dim}(\mathcal{P}) - \text{rank}(QV).
% \]
% Let \( w \) be the sparse representation that binds at least \( \text{dim}(\mathcal{P}) - \text{rank}(QV) \) constraints of \( A_S \). Then, \( w \) is the **unique element** in \( \mathcal{P} \cap \mathcal{T} \) such that \( x = Qw \), where \( \mathcal{T} \) is the subspace defined by the binding constraints.

% - This theorem proves that the sparse solution \( w \) is **locally unique** when restricted to the subspace of binding constraints.

% #### **Final Result:**
% The local uniqueness property provides a key insight into the **sparsity** of solutions to the polyhedral regression problem. It guarantees that if enough constraints are bound, the solution is unique within a restricted subspace, which is critical for developing efficient optimization algorithms.

% ---

% ### **Inclusion in the Report:**

% 1. **Definition of Polyhedrons and Constraints**:  
%    - Define the polyhedron \( \mathcal{P} \) and explain how binding constraints reduce the degrees of freedom in the solution space.
   
% 2. **Existence of Sparse Solutions (Theorem 3.5)**:  
%    - Use **Theorem 3.5** to explain how a sparse solution exists by binding many constraints in the polyhedron. Discuss the importance of dimensional reduction to achieve sparsity.
   
% 3. **Local Uniqueness (Theorem 3.8)**:  
%    - Highlight the local uniqueness property, which ensures that the sparse solution is unique in the subspace defined by the binding constraints.

% 4. **Practical Implications**:  
%    - This theoretical result justifies the use of active-set methods in optimization algorithms, as they converge to the unique sparse solution by correctly identifying the binding constraints.

% These results form the backbone of the proposed method's efficiency and accuracy, providing the theoretical foundation for the new solver's speed and performance improvements.